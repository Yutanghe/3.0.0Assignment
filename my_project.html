<html>
<html lang="en">
<head>
<title>Project: Emotional Input Technology</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
* {
  box-sizing: border-box;
}
table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
}
body {
  font-family: Arial, Helvetica, sans-serif;
}
header {
  background-color: rgb(50, 50, 50);
  padding: 30px;
  text-align: center;
  font-size: 35px;
  color: white;
}
section {
  display: -webkit-flex;
  display: flex;
}
nav {
  -webkit-flex: 1;
  -ms-flex: 1;
  flex: 1;
  background: #ccc;
  padding: 20px;
}
nav ul {
  list-style-type: none;
  text-align: center;
  padding: 0;
}
aside {
  -webkit-flex: 3;
  -ms-flex: 3;
  flex: 3;
  background-color: #fff;
  padding: 10px;
}
article {
  -webkit-flex: 3;
  -ms-flex: 3;
  flex: 3;
  background-color: #f1f1f1;
  padding: 10px;
}
footer {
  background-color: #777;
  padding: 10px;
  text-align: center;
  color: white;
}
@media (max-width: 600px) {
section {
  -webkit-flex-direction: column;
  flex-direction: column;
  }
}
</style>
</head>

<body>
 <header>
  <h1>Implementing Emotional Input Technology</h1>
 </header>

 <section>
  <nav>
    <ul>
      <li><a href ="index.html"> This is link to Index page </a></li>

    </ul>
  </nav>
  <article>
    <h2>Overview</h2>
    <p>
      This project aims to explore the possibility of implementing inputting emotions technologies in the existing applications.
      This project first explain its motivation,
      after that, discuss the required features for some plausible applications,
      follow by mentioning requirements of technologies,
      finally predicts the outcome of the project upon success.
    </p>
    <h2>Motivation</h2>
    <p>
      Emotion input has already been implemented, but in very limited ways;
      Video games asks player feels; Angry, sad, happy, or neutral?
      Movie surveys asks how the audiences feel about a certain scene, etc.
      But the input emotions lacks accuracy, speed and useful results.
      Despite its flaws, emotion input is still used widly today.
    </p>
    <p>
      Which raises the question to me:
    </p>
    <p>
      How plausible is it to implement emotion input technology to our daily applications?
    </p>
    <h2>Feature Description</h2>
     <p>
      Emotion reading technology can be implemeted in many applications.
      Here we will highlights some of the possible applications and dissuss it's required features.
     </p>
     <p>
      Vechicle applications: a beep from the dashboard of a car when driver gets drowsy.
      For this applications, the most prioritised features is speed.
      Given the situation, it needs to warning to the driver as quick as possible.
     </p>
     <p>
      Music applications: people reacting to music and give recommendations based on people's reactions.
      For this applications, the most prioritised features is variety.
      Given the situation, it needs to pick up as many emotions as possible to puzzle useful data.
     </p>
     <p>
      Advertisment applications: advertising research uses emotion technology to study the visceral, subconscious reactions and behavours of consumer.
      For this applications, the most prioritised features is variety.
      Given the situation, it needs to pick up as many emotions as possible to puzzle useful data.
     </p>
     <p>
      Mental health applications: that monitors heartbeat to tell whether patients are experiencing stress, pain, or frustration.
      For this applications, the most prioritised features is accuracy.
      Given the situation, it needs to diagnose patients with precisions.
     </p>
     <p>
      Call center applications: helps call center agents identify the moods of customers on the phone and adjust how they handle the conversation in real time.
      For this applications, the most prioritised features is speed.
      Given the situation, it needs to give the agent rapid update to the calles's emotions response.
     </p>
     <p>
      Communication assist: thoes who suffers communicative prostheses to helping them read other’s facial expressions.
      For this applications, the most prioritised features is accuracy.
      Given the situation, it needs to replicate real emotions of a human.
     </p>

     <h2>Tools and Technologies</h2>
     <p>
      Technological requirement for emotions varies as each applications offers differrent features.
      Here we list some of the possible devices:

      FPGA: FPGA is an embedded devices for human emotion recognition from live cameras.
      Detect emotions: Fear, anger, disgust, contempt, happyness, Supprise, sadness and joy
      Accuracy: 47.44%.

      Heart-Rate Variability: A device that detects emotions through heart-rate.
      Detect emotions: neutral, affection, amusement, anger, disgust, fear and sadness.
      Accuracy: 82.4%

      CNN: A device that detects emotions through speech.
      Detect emotions: Angry, happy and sad
      Accuracy: 66.1%
     </p>
     <h2>Outcome</h2>
     <p>
      Upon the success,
      given the problem is that emotion input does not have speed, variaty and accurate data,
      the project solved all the problems.
      Industries that is heavly focused on emotion input has a high probable to significantly improved.
     </p>



  </article>
 </section>

 <footer>
  <ul>
    <li><a href="https://click.endnote.com/viewer?doi=10.1007%2F978-3-030-19591-5_26&token=WzMzNjQ5MjMsIjEwLjEwMDcvOTc4LTMtMDMwLTE5NTkxLTVfMjYiXQ.YWZ8q7H-X5pqbJ5e-xUPdfy_76E">Fernández-Aguilar, L., Martínez-Rodrigo, A., Moncho-Bogani, J., Fernández-Caballero, A., Latorre, J.M., 2019. Emotion Detection in Aging Adults Through Continuous Monitoring of Electro-Dermal Activity and Heart-Rate Variability, in: Lecture Notes in Computer Science. Lecture Notes in Computer Science, pp. 252–261.. doi:10.1007/978-3-030-19591-5_26</a></li>
    <li><a href="https://click.endnote.com/viewer?doi=10.1109%2Ficassp.2017.7953131&token=WzMzNjQ5MjMsIjEwLjExMDkvaWNhc3NwLjIwMTcuNzk1MzEzMSJd.gdDhIcsxX2p012iOsu1uj7RzWEY">Bertero, D., Fung, P., 2017. A first look into a Convolutional Neural Network for speech emotion detection, in: .. doi:10.1109/icassp.2017.7953131</a></li>
    <li><a href="https://click.endnote.com/viewer?doi=10.3390/technologies6010017">Turabzadeh, S., Meng, H., Swash, R., Pleva, M., Juhar, J., 2018. Facial Expression Emotion Detection for Real-Time Embedded Systems. Technologies 6, 17.. doi:10.3390/technologies6010017</a></li>
  </ul>
 </footer>
</body>
</html>
